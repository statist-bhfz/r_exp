---
title: "Понижение размерности: PCA, MDS, t-SNE"
output: html_document
---

```{r, echo = FALSE}
setwd("D:/GITHUB/r_exp/dim_red")
```

Методы понижения размерности ([обзор](https://www.tilburguniversity.edu/upload/59afb3b8-21a5-4c78-8eb3-6510597382db_TR2009005.pdf)) играют важную роль в машинном обучении. Они позволяют строить модели в пространствах меньшей размерности, чем исходное признаковое пространство, с минимальными потерями информации. Особенно полезно понижать размерность до 2, то есть проецировать данные на плоскость. Таким образом можно изучить структуру данных, например, посмотреть, насколько разделимы классы в задачах классификации. В этом сообщении рассмотрены два классических метода - метод главных компонент (PCA) и многомерное шкалирование (MDS), а также один из мощнейших современных методов - [t-SNE](https://lvdmaaten.github.io/tsne/) (t-Distributed Stochastic Neighbor Embedding).

В качестве примера рассматривается набор данных `Colon` из пакета **plsgenomics**. Он содержит 2000 переменных, характеризующих уровень экспрессии генов в 62 образцах тканей (40 опухолевых и 22 нормальных); решить нужно задачу двухклассовой классификации. Здесь n\<\<p - переменных гораздо больше, чем наблюдений. Попробуем понизить размерность данных разными методами и сравнить результаты.

```{r, message = FALSE}
library(plsgenomics)
library(Rtsne)
library(caret)
data(Colon)
df <- Colon$X # предикторы
Y <- as.factor(Colon$Y) # метки классов
preProcValues <- preProcess(df, method = c("center", "scale")) # стандартизация
df <- predict(preProcValues, df) 
head(df[, 1:5])
```

### PCA

Применим метод главных компонент, используя предварительно стандартизированные данные. Всего будет создано n-1 ненулевых главных компонент, в данном случае 61 в соответствии с фактическй размерностью данных (см. [1](http://stats.stackexchange.com/questions/28909/pca-when-the-dimensionality-is-greater-than-the-number-of-samples), [2](http://stats.stackexchange.com/questions/123318/why-are-there-only-n-1-principal-components-for-n-data-points-if-the-number), [3](http://stats.stackexchange.com/questions/46475/is-pca-appropriate-when-np)).

```{r}
df.pca <- prcomp(df, center = FALSE, scale = FALSE)
plot(df.pca$x[,1], df.pca$x[, 2], col = Y)
```

На графике в пространстве первух двух главных компонент классы выглядят неразделемыми. Это довольно предсказуемо, ведь направления наибольшей вариабельности данных вовсе не обязательно будут тесно связаны с переменной отклика.

### MDS

Многомерное шкалирование ищет проекцию данных в пространство меньшей размерности таким образом, чтобы как можно точнее сохранить расстояния между объектами. Двухмерная проекция, полученная данным методом, выглядит следующим образом:

```{r}
df.mds <- cmdscale(dist(df), eig = TRUE, k = 2) 
plot(df.mds$points[, 1], df.mds$points[, 2], col = Y)
```

Классы по-прежнему выглядят плохо разделимыми.


### t-SNE

Суть метода подробно объясняется в публикации на [хабре](https://habrahabr.ru/post/267041/). Воспользуемся эффективной реализацией алгоритма на C++ при помощи пакета [Rstne](https://github.com/jkrijthe/Rtsne) (есть также [реализация](https://lvdmaaten.github.io/tsne/) непосредственно на R):

```{r}
set.seed(100)
df.rtsne <- Rtsne(Colon$X, dims = 2, perplexity = 20)
plot(df.rtsne$Y, col = Y)
```

В полученном пространстве классы можно достаточно хорошо разделить с использованием простейших методов, таких как одиночное решающее дерево. Недостаток метода состоит в том, что решаемая им задача оптимизации не имеет единственного решения, а полученные переменные, в координатах которых строится график, не выражаются через исходные переменные. Если на обучающей выборке построить модель, то она окажется бесполезной, поскольку нельзя определить координаты новых точек в пространстве пониженной размерности. Иными словами, мы получили информацию о разделимости классов, но по-прежнему не знаем, как именно их разделить. 

Очевидным выходом из ситуации является предложенный ниже алгоритм:

1) Для обучающей выборки понизить размерность (например, до 2) при помощи t-SNE.

2) Подобрать оптимальную модель с использованием кросс-валидации.

3) Новые данные, для которых неизвестны значения целевой переменной, объединить с обучающей выборкой. Получившаяся выборка будет содержать размеченную и неразмеченную часть.

4) Аналогичным образом понизить размерность для новой выборки, включающей в себя обучающую.

5) На размеченной части выборки построить модель с подобранной в п.2 спецификацией. 

6) Использовать модель для предсказания значений целевой переменной на неразмеченной части выборки. 

Иллюстрация на примере k ближайших соседей:

```{r}
set.seed(100)
df.rtsne <- Rtsne(Colon$X, dims = 2, perplexity = 20)$Y
df.rtsne <- cbind(df.rtsne, Colon$Y)
df.rtsne <- as.data.frame(df.rtsne)
df.rtsne[, 3] <- factor(df.rtsne[, 3], levels = c(1, 2), 
                        labels = c("normal", "tumor"))
colnames(df.rtsne) <- c("X1", "X2", "Y")
trainIndex <- createDataPartition(df.rtsne$Y, p = 0.8, list = FALSE, times = 1)
train <- df.rtsne[trainIndex, ]
test <- df.rtsne[-trainIndex, ]

set.seed(100)
fitControl <- trainControl(method = "cv",
                           number = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)
set.seed(100)
fit1 <- train(Y ~ X1 + X2,
              data = train,
              method = "knn", 
              trControl = fitControl,
              metric = "ROC")
fit1

pred <- predict(fit1, newdata = test)
confusionMatrix(pred, test$Y, positive = "tumor")
```

P.S. [Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning](http://www.cs.toronto.edu/~zemel/documents/knca_camera.pdf)